<!DOCTYPE html>
<html lang="es">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Cómo Dotar a tu IA Local (Ollama) de "Superpoderes": Ejecutando Funciones Personalizadas</title>
    <style>
        body {
            font-family: -apple-system, BlinkMacSystemFont, "Segoe UI", Roboto, Helvetica, Arial, sans-serif, "Apple Color Emoji", "Segoe UI Emoji", "Segoe UI Symbol";
            line-height: 1.6;
            margin: 0;
            padding: 0;
            background-color: #f4f4f4;
            color: #333;
        }
        .container {
            width: 80%;
            max-width: 900px;
            margin: 20px auto;
            background-color: #fff;
            padding: 20px 30px;
            box-shadow: 0 0 10px rgba(0,0,0,0.1);
            border-radius: 8px;
        }
        h1, h2, h3 {
            color: #2c3e50;
            margin-top: 1.5em;
        }
        h1 {
            text-align: center;
            border-bottom: 2px solid #3498db;
            padding-bottom: 0.5em;
            margin-top: 0;
        }
        h2 {
            border-bottom: 1px solid #eee;
            padding-bottom: 0.3em;
        }
        code {
            font-family: "SFMono-Regular", Consolas, "Liberation Mono", Menlo, Courier, monospace;
            background-color: #eef;
            padding: 2px 5px;
            border-radius: 4px;
            font-size: 0.9em;
        }
        pre {
            background-color: #2d2d2d; /* Dark background for code blocks */
            color: #f8f8f2; /* Light text for code blocks */
            padding: 15px;
            border-radius: 5px;
            overflow-x: auto;
            font-size: 0.9em;
            line-height: 1.4;
        }
        pre code {
            background-color: transparent;
            padding: 0;
            border-radius: 0;
            font-size: 1em; /* Ensure code inside pre is not doubly shrunk */
        }
        /* Basic syntax highlighting - can be expanded */
        .python .hljs-keyword { color: #f92672; }
        .python .hljs-string { color: #e6db74; }
        .python .hljs-comment { color: #75715e; }
        .python .hljs-function { color: #a6e22e; }
        .python .hljs-params { color: #fd971f; }
        .python .hljs-number { color: #ae81ff; }
        .python .hljs-class { color: #66d9ef; }


        .note {
            background-color: #e7f3fe;
            border-left: 6px solid #2196F3;
            padding: 15px;
            margin: 20px 0;
        }
        .warning {
            background-color: #fff3cd;
            border-left: 6px solid #ffc107;
            padding: 15px;
            margin: 20px 0;
        }
        ul {
            list-style-type: disc;
            margin-left: 20px;
        }
        li {
            margin-bottom: 0.5em;
        }
        a {
            color: #3498db;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1>Cómo Dotar a tu IA Local (Ollama) de "Superpoderes": Ejecutando Funciones Personalizadas</h1>

        <p>¡Bienvenido/a a este nuevo artículo de <strong>entendiendo-ai</strong>! Hoy vamos a sumergirnos en una capacidad fascinante de los modelos de lenguaje grandes (LLMs): la habilidad de no solo generar texto, sino de entender intenciones y ejecutar acciones concretas en tu sistema. Específicamente, te enseñaremos cómo puedes hacer que tu modelo local Ollama interactúe con funciones Python personalizadas que tú mismo crearás.</p>
        <p>Imagina poder pedirle a tu IA que cree un archivo Excel con una nota, busque algo en Google por ti, o te cuente un dato curioso obtenido de una API pública, ¡todo directamente desde la interfaz de chat y ejecutándose localmente! Esto abre un mundo de posibilidades para crear agentes de IA mucho más útiles y versátiles, manteniendo la privacidad y el control que ofrecen los modelos locales.</p>
        <p>En este tutorial, te guiaremos paso a paso para:
            <ul>
                <li>Definir funciones en Python que realicen tareas como crear archivos, abrir aplicaciones (navegador) y consultar APIs.</li>
                <li>Diseñar un prompt para que Ollama entienda cuándo llamar a estas funciones y con qué datos.</li>
                <li>Crear un script "orquestador" en Python que conecte Ollama con tus funciones.</li>
            </ul>
        </p>
        <p>¡Prepárate para darle superpoderes a tu IA!</p>

        <h2>¿Qué es la Ejecución de Funciones (Function Calling) en Modelos de Lenguaje?</h2>
        <p>Cuando interactúas con un LLM estándar, le das un texto (prompt) y él te devuelve otro texto. La ejecución de funciones, a menudo llamada "function calling" o "tool use", lleva esto un paso más allá. En lugar de solo generar texto, el LLM puede ser instruido para reconocer que una solicitud del usuario corresponde a una tarea específica para la cual existe una "herramienta" o "función" disponible.</p>
        <p>El proceso generalmente involucra:</p>
        <ol>
            <li><strong>Detección de Intención:</strong> El LLM analiza la entrada del usuario para entender si la petición puede ser resuelta por alguna de las funciones que conoce.</li>
            <li><strong>Extracción de Parámetros:</strong> Si el LLM identifica una función relevante, extrae de la conversación los datos (argumentos) necesarios para ejecutar esa función (por ejemplo, el nombre de un archivo, un término de búsqueda). Si la función no requiere parámetros, este paso se omite para esa función.</li>
            <li><strong>Formato de Salida:</strong> El LLM devuelve una respuesta estructurada (comúnmente en formato JSON) que indica qué función se debe llamar y con qué argumentos (si los hay).</li>
        </ol>
        <div class="note">
            <p><strong>Importante:</strong> El LLM <em>no ejecuta directamente</em> el código de la función. Actúa como un cerebro que decide qué hacer y con qué información. Un código externo (que crearemos en Python) es el que realmente interpreta la decisión del LLM y ejecuta la función correspondiente.</p>
        </div>

        <h2>Requisitos Previos</h2>
        <p>Antes de comenzar, asegúrate de tener lo siguiente:</p>
        <ul>
            <li><strong>Ollama instalado y un modelo descargado:</strong> Si aún no lo tienes, puedes seguir guías en la web oficial de Ollama. Para este tutorial, modelos como <code>llama3</code>, <code>mistral</code> o <code>phi3</code> funcionarán bien.</li>
            <li><strong>Python instalado:</strong> Recomendamos Python 3.7 o superior.</li>
            <li><strong>Librerías de Python:</strong> Necesitarás instalar algunas librerías. Puedes hacerlo abriendo tu terminal o línea de comandos y ejecutando:
                <pre><code class="bash">pip install ollama openpyxl requests</code></pre>
                <ul>
                    <li><code>ollama</code>: Para interactuar con tu instancia local de Ollama desde Python.</li>
                    <li><code>openpyxl</code>: Para crear y manipular archivos Excel.</li>
                    <li><code>requests</code>: Para realizar solicitudes HTTP (la usaremos para la API de datos sobre gatos).</li>
                    <li>La librería <code>webbrowser</code> (para abrir el navegador) generalmente viene incluida con Python.</li>
                </ul>
            </li>
        </ul>

        <h2>Paso 1: Definiendo Nuestras Funciones Personalizadas en Python</h2>
        <p>Lo primero es crear las funciones en Python que queremos que nuestra IA pueda "llamar". Usaremos las tres funciones que has definido, cubriendo la creación de archivos, la apertura de aplicaciones y la consulta de APIs:</p>

        <pre><code class="python">
# funciones_custom.py
import webbrowser
import openpyxl
import requests
import json # Aunque no lo usemos directamente en obtener_hecho_gatos_api, es bueno tenerlo si otras APIs lo necesitaran.

def crear_excel_con_frase(frase: str, nombre_archivo: str = "documento_creado.xlsx") -> str:
    """
    Crea un archivo Excel (.xlsx) con una frase específica en la celda A1.

    :param frase: El texto que se escribirá en el Excel.
    :param nombre_archivo: El nombre con el que se guardará el archivo Excel.
    :return: Mensaje de confirmación.
    """
    try:
        workbook = openpyxl.Workbook()
        sheet = workbook.active
        sheet["A1"] = frase
        workbook.save(nombre_archivo)
        return f"Excel '{nombre_archivo}' creado exitosamente con la frase: '{frase}'."
    except Exception as e:
        return f"Error al crear el Excel: {e}"

def abrir_chrome_y_buscar(termino_busqueda: str) -> str:
    """
    Abre el navegador web predeterminado y realiza una búsqueda en Google.

    :param termino_busqueda: Lo que el usuario quiere buscar.
    :return: Mensaje de confirmación.
    """
    try:
        url = f"https://www.google.com/search?q={termino_busqueda.replace(' ', '+')}"
        webbrowser.open_new_tab(url)
        return f"Abriendo navegador y buscando en Google: '{termino_busqueda}'."
    except Exception as e:
        return f"Error al intentar abrir el navegador o buscar: {e}"

def obtener_hecho_gatos_api() -> str:
    """
    Obtiene un hecho curioso aleatorio sobre gatos desde la API catfact.ninja.
    Esta API no requiere autenticación.
    """
    api_url = "https://catfact.ninja/fact"
    # print(f"Consultando la URL: {api_url}") # Descomentar para depuración

    try:
        response = requests.get(api_url, timeout=10) # Añadido timeout
        response.raise_for_status() 
        data = response.json()
        hecho = data.get("fact")
        longitud = data.get("length")

        if hecho:
            return f"Hecho sobre gatos: {hecho} (Longitud: {longitud} caracteres)"
        else:
            return "La API de gatos respondió, pero el formato del dato no es el esperado."
    except requests.exceptions.HTTPError as http_err:
        return f"Error HTTP al consultar la API de Cat Facts: {http_err} (Status: {response.status_code if 'response' in locals() else 'N/A'})"
    except requests.exceptions.ConnectionError as conn_err:
        return f"Error de conexión con la API de Cat Facts: {conn_err}"
    except requests.exceptions.Timeout as timeout_err:
        return f"Error de Timeout consultando la API de Cat Facts: {timeout_err}"
    except requests.exceptions.RequestException as req_err:
        return f"Error en la petición a la API de Cat Facts: {req_err}"
    except ValueError as json_err: # Si la respuesta no es JSON
        return f"Error al procesar el JSON de la API de Cat Facts: {json_err}"
    except Exception as e:
        return f"Ocurrió un error inesperado con la API de Cat Facts: {e}"

# Ejemplos de uso (para probar las funciones directamente):
# if __name__ == "__main__":
#     print(crear_excel_con_frase("Hola desde el artículo!", "mi_excel_gatos.xlsx"))
#     print(abrir_chrome_y_buscar("imagenes de gatos graciosos"))
#     print(obtener_hecho_gatos_api())
        </code></pre>
        <p><strong>Explicación de las Funciones:</strong></p>
        <ul>
            <li><strong><code>crear_excel_con_frase</code></strong>: Usa <code>openpyxl</code> para crear un archivo <code>.xlsx</code> con una frase dada en la celda A1.</li>
            <li><strong><code>abrir_chrome_y_buscar</code></strong>: Usa <code>webbrowser</code> para abrir una nueva pestaña en el navegador predeterminado con una búsqueda de Google.</li>
            <li><strong><code>obtener_hecho_gatos_api</code></strong>: Usa <code>requests</code> para consultar la API pública `https://catfact.ninja/fact`, que devuelve un dato curioso sobre gatos en formato JSON. Esta función no requiere parámetros de entrada. He añadido un manejo de errores más detallado y un `timeout` a la petición.</li>
        </ul>
        <p>Guarda este código en un archivo llamado <code>funciones_custom.py</code>.</p>

        <h2>Paso 2: Diseñando el "Prompt" para la Detección de Intención y Extracción de Parámetros</h2>
        <p>Ahora, ajustaremos el prompt del sistema para que el LLM conozca estas tres funciones específicas.</p>

        <p><strong>Estrategia de Prompting:</strong></p>
        <ul>
            <li>Daremos descripciones claras de `crear_excel_con_frase`, `abrir_chrome_y_buscar` y `obtener_hecho_gatos_api`.</li>
            <li>Especificaremos que `obtener_hecho_gatos_api` no necesita argumentos.</li>
            <li>Mantendremos el formato de salida JSON.</li>
        </ul>

        <p>Aquí tienes el prompt actualizado:</p>
        <pre><code class="text">
Eres un asistente IA útil y eficiente. Tu tarea es analizar la solicitud del usuario.
Si la solicitud del usuario puede ser resuelta por una de las funciones que te describo a continuación,
debes responder *únicamente* con un objeto JSON que especifique la función a llamar y sus argumentos (si los tiene).
No añadas explicaciones, saludos, ni ningún otro texto fuera del objeto JSON.
Si la solicitud del usuario no se corresponde claramente con ninguna de las funciones disponibles, o si es una pregunta general,
simplemente responde a la pregunta o continúa la conversación normalmente sin generar un JSON.

Funciones disponibles:
1.  **crear_excel_con_frase**:
    * Descripción: Crea un archivo Excel (.xlsx) con una frase específica en la celda A1. Útil para guardar notas rápidas o datos en formato de hoja de cálculo.
    * Parámetros:
        * `frase` (string, obligatorio): El texto que se escribirá en la celda A1 del Excel.
        * `nombre_archivo` (string, opcional, por defecto "documento_creado.xlsx"): El nombre deseado para el archivo Excel. Debe terminar en .xlsx.

2.  **abrir_chrome_y_buscar**:
    * Descripción: Abre el navegador web predeterminado y realiza una búsqueda en Google con el término proporcionado. Útil para encontrar información en la web.
    * Parámetros:
        * `termino_busqueda` (string, obligatorio): El texto o la pregunta que el usuario quiere buscar en Google.

3.  **obtener_hecho_gatos_api**:
    * Descripción: Obtiene un hecho curioso aleatorio sobre gatos desde una API pública. Útil cuando el usuario quiere saber algo divertido o aleatorio sobre gatos.
    * Parámetros: Esta función no requiere ningún parámetro.

Ejemplos de cómo debes responder:

Si el usuario dice: "Crea un excel que diga 'Recordatorio importante' y llámalo recordatorio.xlsx"
Tu respuesta DEBE SER:
{"funcion_a_llamar": "crear_excel_con_frase", "argumentos": {"frase": "Recordatorio importante", "nombre_archivo": "recordatorio.xlsx"}}

Si el usuario dice: "Busca en Google fotos de gatitos"
Tu respuesta DEBE SER:
{"funcion_a_llamar": "abrir_chrome_y_buscar", "argumentos": {"termino_busqueda": "fotos de gatitos"}}

Si el usuario dice: "Cuéntame un dato curioso sobre gatos" o "Dime algo sobre los gatos"
Tu respuesta DEBE SER:
{"funcion_a_llamar": "obtener_hecho_gatos_api", "argumentos": {}}

Si el usuario dice: "Hola, ¿cómo estás?" o "¿Cuál es la capital de Argentina?"
Debes responder de forma conversacional, por ejemplo: "¡Hola! Estoy bien, ¿en qué puedo ayudarte?" o "La capital de Argentina es Buenos Aires."
        </code></pre>

        <h2>Paso 3: El Orquestador en Python - Conectando Ollama con Nuestras Funciones</h2>
        <p>Ahora actualizaremos el script `agente_ollama.py` para que trabaje con estas nuevas funciones y el prompt modificado. Notablemente, eliminaremos cualquier referencia a la API key de OpenWeatherMap.</p>

        <p>Crea/modifica tu archivo <code>agente_ollama.py</code> con el siguiente código:</p>
        <pre><code class="python">
# agente_ollama.py
import ollama
import json
import funciones_custom # Importamos nuestro módulo con las funciones

# Configuración del modelo de Ollama a usar
MODELO_OLLAMA = "llama3" # Puedes cambiarlo por 'mistral', 'phi3', etc.

# Prompt del sistema detallado (actualizado para las nuevas funciones)
SYSTEM_PROMPT = """
Eres un asistente IA útil y eficiente. Tu tarea es analizar la solicitud del usuario.
Si la solicitud del usuario puede ser resuelta por una de las funciones que te describo a continuación,
debes responder *únicamente* con un objeto JSON que especifique la función a llamar y sus argumentos (si los tiene).
No añadas explicaciones, saludos, ni ningún otro texto fuera del objeto JSON.
Si la solicitud del usuario no se corresponde claramente con ninguna de las funciones disponibles, o si es una pregunta general,
simplemente responde a la pregunta o continúa la conversación normalmente sin generar un JSON.

Funciones disponibles:
1.  **crear_excel_con_frase**:
    * Descripción: Crea un archivo Excel (.xlsx) con una frase específica en la celda A1. Útil para guardar notas rápidas o datos en formato de hoja de cálculo.
    * Parámetros:
        * `frase` (string, obligatorio): El texto que se escribirá en la celda A1 del Excel.
        * `nombre_archivo` (string, opcional, por defecto "documento_creado.xlsx"): El nombre deseado para el archivo Excel. Debe terminar en .xlsx.

2.  **abrir_chrome_y_buscar**:
    * Descripción: Abre el navegador web predeterminado y realiza una búsqueda en Google con el término proporcionado. Útil para encontrar información en la web.
    * Parámetros:
        * `termino_busqueda` (string, obligatorio): El texto o la pregunta que el usuario quiere buscar en Google.

3.  **obtener_hecho_gatos_api**:
    * Descripción: Obtiene un hecho curioso aleatorio sobre gatos desde una API pública. Útil cuando el usuario quiere saber algo divertido o aleatorio sobre gatos.
    * Parámetros: Esta función no requiere ningún parámetro.

Ejemplos de cómo debes responder:

Si el usuario dice: "Crea un excel que diga 'Recordatorio importante' y llámalo recordatorio.xlsx"
Tu respuesta DEBE SER:
{"funcion_a_llamar": "crear_excel_con_frase", "argumentos": {"frase": "Recordatorio importante", "nombre_archivo": "recordatorio.xlsx"}}

Si el usuario dice: "Busca en Google fotos de gatitos"
Tu respuesta DEBE SER:
{"funcion_a_llamar": "abrir_chrome_y_buscar", "argumentos": {"termino_busqueda": "fotos de gatitos"}}

Si el usuario dice: "Cuéntame un dato curioso sobre gatos" o "Dime algo sobre los gatos"
Tu respuesta DEBE SER:
{"funcion_a_llamar": "obtener_hecho_gatos_api", "argumentos": {}}

Si el usuario dice: "Hola, ¿cómo estás?" o "¿Cuál es la capital de Argentina?"
Debes responder de forma conversacional, por ejemplo: "¡Hola! Estoy bien, ¿en qué puedo ayudarte?" o "La capital de Argentina es Buenos Aires."
"""

def procesar_respuesta_llm(respuesta_llm_texto):
    """
    Intenta parsear la respuesta del LLM como JSON.
    Si tiene el formato esperado para una llamada a función, devuelve los detalles.
    De lo contrario, devuelve None.
    """
    try:
        if "```json" in respuesta_llm_texto:
            json_str = respuesta_llm_texto.split("```json")[1].split("```")[0].strip()
        elif respuesta_llm_texto.strip().startswith("{") and respuesta_llm_texto.strip().endswith("}"):
            json_str = respuesta_llm_texto.strip()
        else:
            return None

        data = json.loads(json_str)
        if "funcion_a_llamar" in data and "argumentos" in data:
            return data
    except json.JSONDecodeError:
        pass
    except Exception:
        pass
    return None

def main():
    print(f"Iniciando chat con el modelo {MODELO_OLLAMA}. Escribe 'salir' para terminar.")
    print("Asegúrate de que Ollama esté corriendo y el modelo esté disponible.\n")

    mensajes = [{"role": "system", "content": SYSTEM_PROMPT}]

    while True:
        try:
            user_input = input("Tú: ")
            if user_input.lower() == 'salir':
                print("Saliendo...")
                break

            mensajes.append({"role": "user", "content": user_input})

            response = ollama.chat(
                model=MODELO_OLLAMA,
                messages=mensajes
            )
            respuesta_llm_texto = response['message']['content']
            
            # Siempre añadimos la respuesta del LLM al historial para mantener contexto
            # independientemente de si es una llamada a función o una respuesta directa.
            mensajes.append({"role": "assistant", "content": respuesta_llm_texto})
            
            # Limitar el historial para no exceder límites de contexto (opcional, pero buena práctica)
            if len(mensajes) > 11: # System prompt + 5 pares de user/assistant
                 mensajes = [mensajes[0]] + mensajes[-10:]

            print(f"\nLLM (raw): {respuesta_llm_texto}") # Para depuración

            info_funcion = procesar_respuesta_llm(respuesta_llm_texto)

            if info_funcion:
                nombre_funcion = info_funcion["funcion_a_llamar"]
                argumentos = info_funcion["argumentos"]
                
                resultado_ejecucion = ""
                print(f"LLM quiere llamar a la función: {nombre_funcion} con argumentos: {argumentos}")

                if nombre_funcion == "crear_excel_con_frase":
                    # frase es obligatorio, nombre_archivo es opcional (con default en la función)
                    if "frase" in argumentos:
                        resultado_ejecucion = funciones_custom.crear_excel_con_frase(
                            frase=argumentos.get("frase"),
                            nombre_archivo=argumentos.get("nombre_archivo") 
                        )
                    else:
                        resultado_ejecucion = "Error: El LLM no proporcionó el parámetro 'frase' para crear_excel_con_frase."

                elif nombre_funcion == "abrir_chrome_y_buscar":
                    if "termino_busqueda" in argumentos:
                        resultado_ejecucion = funciones_custom.abrir_chrome_y_buscar(
                            termino_busqueda=argumentos.get("termino_busqueda")
                        )
                    else:
                        resultado_ejecucion = "Error: El LLM no proporcionó el parámetro 'termino_busqueda' para abrir_chrome_y_buscar."
                
                elif nombre_funcion == "obtener_hecho_gatos_api":
                    # Esta función no toma argumentos, así que la llamamos directamente.
                    resultado_ejecucion = funciones_custom.obtener_hecho_gatos_api()
                
                else:
                    resultado_ejecucion = f"Error: Función '{nombre_funcion}' no reconocida por el orquestador."
                
                print(f"Resultado de la función: {resultado_ejecucion}\n")
                # Opcional: añadir el resultado de la función al historial para que el LLM lo sepa
                # mensajes.append({"role": "tool", "tool_call_id": "algún_id_si_es_necesario", "name": nombre_funcion, "content": resultado_ejecucion})
                # O más simple:
                # mensajes.append({"role": "assistant", "content": f"Resultado de {nombre_funcion}: {resultado_ejecucion}"})


            else:
                # Si no es una llamada a función, la respuesta ya fue añadida al historial
                # y simplemente la mostramos al usuario (ya se imprimió como LLM (raw), 
                # podríamos hacerla más bonita aquí o dejar que el LLM (raw) sea la salida directa)
                print(f"Agente IA: {respuesta_llm_texto}\n") # La respuesta del LLM ya está en `respuesta_llm_texto`

        except ollama.ResponseError as e:
            print(f"Error de respuesta de Ollama: {e.error}")
            if hasattr(e, 'response') and e.response is not None:
                print(f"Detalles de la respuesta del servidor: {e.response.text}")
        except Exception as e:
            print(f"Ocurrió un error inesperado: {e}")


if __name__ == "__main__":
    try:
        ollama.list() 
        print("Conexión con Ollama exitosa.")
    except Exception as e:
        print(f"Error al conectar con Ollama. Asegúrate de que Ollama esté ejecutándose.")
        print(f"Detalle del error: {e}")
        exit()
    
    main()
        </code></pre>
        <p><strong>Cambios Clave en el Orquestador:</strong></p>
        <ul>
            <li>El <code>SYSTEM_PROMPT</code> ha sido actualizado.</li>
            <li>En el bucle principal, la lógica para llamar a <code>obtener_hecho_gatos_api</code> simplemente la invoca sin pasar argumentos, ya que no los necesita.</li>
            <li>Se han añadido pequeñas validaciones para los argumentos de las otras funciones antes de llamarlas.</li>
            <li>Se eliminaron todas las referencias a <code>OPENWEATHERMAP_API_KEY</code> y las comprobaciones relacionadas.</li>
            <li>Se ajustó ligeramente el manejo del historial de mensajes.</li>
            <li>Se añadió un manejo más específico para `ollama.ResponseError`.</li>
        </ul>

        <h2>Paso 4: ¡Probando Nuestro Agente!</h2>
        <p>Para probar tu agente actualizado:</p>
        <ol>
            <li>Asegúrate de que tu servidor Ollama esté corriendo (<code>ollama serve</code>) y tengas el modelo (ej. <code>llama3</code>) descargado (<code>ollama pull llama3</code>).</li>
            <li>Guarda el código de las funciones en <code>funciones_custom.py</code>.</li>
            <li>Guarda el código del orquestador en <code>agente_ollama.py</code>.</li>
            <li>Abre una terminal en el directorio donde guardaste los archivos y ejecuta: <code>python agente_ollama.py</code></li>
        </ol>

        <p>Ahora puedes interactuar. Prueba con frases como:</p>

        <p><strong>Ejemplo 1: Crear un Excel</strong></p>
        <p><code>Tú: Quiero guardar la nota 'Comprar leche y pan' en un excel llamado lista_super.xlsx</code></p>
        <p><em>Salida esperada del LLM (mostrada para depuración):</em></p>
        <pre><code>
LLM (raw): {"funcion_a_llamar": "crear_excel_con_frase", "argumentos": {"frase": "Comprar leche y pan", "nombre_archivo": "lista_super.xlsx"}}
        </code></pre>
        <p><em>Salida del orquestador:</em></p>
        <pre><code>
LLM quiere llamar a la función: crear_excel_con_frase con argumentos: {'frase': 'Comprar leche y pan', 'nombre_archivo': 'lista_super.xlsx'}
Resultado de la función: Excel 'lista_super.xlsx' creado exitosamente con la frase: 'Comprar leche y pan'.
        </code></pre>

        <p><strong>Ejemplo 2: Buscar en Google</strong></p>
        <p><code>Tú: Busca en internet cómo hacer una torta de chocolate</code></p>
        <p><em>Salida esperada del LLM:</em></p>
        <pre><code>
LLM (raw): {"funcion_a_llamar": "abrir_chrome_y_buscar", "argumentos": {"termino_busqueda": "cómo hacer una torta de chocolate"}}
        </code></pre>
        <p><em>Salida del orquestador:</em></p>
        <pre><code>
LLM quiere llamar a la función: abrir_chrome_y_buscar con argumentos: {'termino_busqueda': 'cómo hacer una torta de chocolate'}
Resultado de la función: Abriendo navegador y buscando en Google: 'cómo hacer una torta de chocolate'.
        </code></pre>

        <p><strong>Ejemplo 3: Obtener Dato Curioso de Gatos</strong></p>
        <p><code>Tú: cuéntame algo sobre los gatos</code></p>
        <p><em>Salida esperada del LLM:</em></p>
        <pre><code>
LLM (raw): {"funcion_a_llamar": "obtener_hecho_gatos_api", "argumentos": {}}
        </code></pre>
        <p><em>Salida del orquestador:</em></p>
        <pre><code>
LLM quiere llamar a la función: obtener_hecho_gatos_api con argumentos: {}
Resultado de la función: Hecho sobre gatos: A cat's field of vision is about 200 degrees. (Longitud: 46 caracteres) (El resultado real variará)
        </code></pre>

        <p><strong>Ejemplo 4: Conversación Normal</strong></p>
        <p><code>Tú: ¿Qué día es hoy?</code></p>
        <p><em>(Asumiendo que el modelo no está configurado para saber la fecha actual por sí mismo de forma precisa sin una función)</em></p>
        <p><em>Salida esperada del LLM (directamente, sin JSON):</em></p>
        <pre><code>
LLM (raw): No tengo acceso a un calendario en tiempo real para decirte la fecha exacta. Soy un modelo de lenguaje.
        </code></pre>
        <p><em>Salida del orquestador:</em></p>
        <pre><code>
Agente IA: No tengo acceso a un calendario en tiempo real para decirte la fecha exacta. Soy un modelo de lenguaje.
        </code></pre>

        <h2>Consideraciones Avanzadas y Próximos Pasos</h2>
        <p>Esta base es sólida. Algunas ideas para expandirlo:</p>
        <ul>
            <li><strong>Manejo de Errores Más Robusto:</strong> Aunque hemos mejorado el manejo de errores en `obtener_hecho_gatos_api`, siempre se puede refinar más la validación de los argumentos que devuelve el LLM para todas las funciones.</li>
            <li><strong>Feedback al LLM sobre la Ejecución:</strong> Podrías enviar el resultado de la función de vuelta al LLM (como un mensaje con rol "tool" o modificando el siguiente mensaje "assistant") para que genere una respuesta más natural. Por ejemplo, "He encontrado este dato sobre gatos para ti: [dato]. ¿Te gustaría otro?".</li>
            <li><strong>Más Funciones:</strong> Piensa en otras tareas que te gustaría automatizar: enviar un correo simple (¡con cuidado!), obtener titulares de noticias de una API RSS, controlar otros scripts locales, etc.</li>
            <li><strong>Seguridad:</strong> Siempre enfatiza la precaución al permitir que un LLM desencadene acciones, especialmente si modifican archivos o interactúan con servicios externos.</li>
        </ul>

        <h2>Conclusión</h2>
        <p>¡Felicidades! Has actualizado y refinado tu agente IA local con Ollama para ejecutar un conjunto diverso de funciones: creación de archivos, apertura de aplicaciones y consulta de APIs públicas sin autenticación. Esto demuestra la flexibilidad y potencia de combinar LLMs locales con código Python personalizado.</p>
        <p>La clave sigue siendo un buen diseño del prompt y un orquestador Python que pueda interpretar las intenciones del LLM y actuar en consecuencia. Al usar una API pública sin clave, también hemos simplificado uno de los pasos, haciendo que el ejemplo sea aún más accesible.</p>
        <p>Te animamos a seguir experimentando. La capacidad de los LLMs para usar herramientas es una de las áreas más activas y emocionantes de la IA actual.</p>
        <p>Esperamos que este artículo te haya sido de gran utilidad. ¡No dudes en dejar tus comentarios, preguntas o compartir los proyectos que construyas con esta base!</p>

    </div>
    <script>
        // JavaScript Mínimo
        // Podrías añadir JavaScript aquí en el futuro para:
        // 1. Resaltado de sintaxis para los bloques de código
        // 2. Interacción con elementos de la página.
        console.log("Artículo de Funciones con Ollama (versión gatos) cargado. Fecha actual: " + new Date().toLocaleDateString());
    </script>
</body>
</html>